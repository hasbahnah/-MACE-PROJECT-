{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow8aZiGNnPGG"
      },
      "source": [
        "# A Short MACE Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlFX3zUWoMdL"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This is a short tutorial for MACE, a highly accurate and efficient ML interatomic potential.\n",
        "Please read the associated [paper](https://arxiv.org/pdf/2206.07697.pdf).\n",
        "The reference implementation is available [here](https://github.com/ACEsuit/mace)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rtvmWR_nZgW"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Icx3drmDnydW",
        "outputId": "a8319cb5-0dc6-444e-bd57-a327632a042f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting e3nn==0.4.4\n",
            "  Downloading e3nn-0.4.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Collecting ase\n",
            "  Downloading ase-3.23.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting torch_ema\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl.metadata (415 bytes)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn==0.4.4) (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn==0.4.4) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from e3nn==0.4.4) (2.5.0+cu121)\n",
            "Collecting opt-einsum-fx>=0.1.4 (from e3nn==0.4.4)\n",
            "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from ase) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase) (3.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn==0.4.4) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn==0.4.4) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn==0.4.4) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn==0.4.4) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->e3nn==0.4.4) (2024.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn==0.4.4) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->e3nn==0.4.4) (3.0.2)\n",
            "Downloading e3nn-0.4.4-py3-none-any.whl (387 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.7/387.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ase-3.23.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: torch_ema, opt-einsum-fx, ase, e3nn\n",
            "Successfully installed ase-3.23.0 e3nn-0.4.4 opt-einsum-fx-0.1.4 torch_ema-0.3\n",
            "Cloning into 'mace'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 104 (delta 3), reused 28 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (104/104), 51.01 MiB | 19.20 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install e3nn==0.4.4 opt_einsum ase torch_ema prettytable\n",
        "\n",
        "# Clone MACE\n",
        "!git clone --depth 1 https://github.com/ACEsuit/mace.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2XNYxlFHEKR",
        "outputId": "79be7820-9cf5-47b1-8e86-ff6c1a862fe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing ./mace\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (2.5.0+cu121)\n",
            "Requirement already satisfied: e3nn==0.4.4 in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (0.4.4)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (3.4.0)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (3.23.0)\n",
            "Requirement already satisfied: torch-ema in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (0.3)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (3.12.0)\n",
            "Collecting matscipy (from mace-torch==0.3.7)\n",
            "  Downloading matscipy-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (37 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (3.12.1)\n",
            "Collecting torchmetrics (from mace-torch==0.3.7)\n",
            "  Downloading torchmetrics-1.5.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting python-hostlist (from mace-torch==0.3.7)\n",
            "  Downloading python-hostlist-2.0.0.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting configargparse (from mace-torch==0.3.7)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (3.1.43)\n",
            "Requirement already satisfied: pyYAML in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (4.66.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (3.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mace-torch==0.3.7) (2.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn==0.4.4->mace-torch==0.3.7) (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn==0.4.4->mace-torch==0.3.7) (1.13.1)\n",
            "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from e3nn==0.4.4->mace-torch==0.3.7) (0.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.7) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.7) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.7) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.7) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12->mace-torch==0.3.7) (2024.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn==0.4.4->mace-torch==0.3.7) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.7) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.7) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.7) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.7) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.7) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.7) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.7) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mace-torch==0.3.7) (2.8.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython->mace-torch==0.3.7) (4.0.11)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mace-torch==0.3.7) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->mace-torch==0.3.7) (2024.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->mace-torch==0.3.7) (0.2.13)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->mace-torch==0.3.7)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython->mace-torch==0.3.7) (5.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics->mace-torch==0.3.7) (75.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mace-torch==0.3.7) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12->mace-torch==0.3.7) (3.0.2)\n",
            "Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Downloading matscipy-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (448 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.8/448.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.5.2-py3-none-any.whl (891 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.4/891.4 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Building wheels for collected packages: mace-torch, python-hostlist\n",
            "  Building wheel for mace-torch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mace-torch: filename=mace_torch-0.3.7-py3-none-any.whl size=139209 sha256=9ed574d8c89831aa79f02aba592076a98518fc3f5078cb04fb56b11e6c73a9d4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a8lm5h6p/wheels/df/5f/32/ef59561725170a81c728fd01c75e56a9ee83bad6da485fc6a5\n",
            "  Building wheel for python-hostlist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-hostlist: filename=python_hostlist-2.0.0-py3-none-any.whl size=39465 sha256=3affcf62841bf4f73fe902b74d3400f56a27ab4a5af2bf925ecb695f41b171e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/e9/0b/1e7a2ca15b702dd36c81c33dad4c23773270dc417eca48dc72\n",
            "Successfully built mace-torch python-hostlist\n",
            "Installing collected packages: python-hostlist, lightning-utilities, configargparse, torchmetrics, matscipy, mace-torch\n",
            "Successfully installed configargparse-1.7 lightning-utilities-0.11.8 mace-torch-0.3.7 matscipy-1.1.1 python-hostlist-2.0.0 torchmetrics-1.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install mace/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdG2dLNXDhVN"
      },
      "source": [
        "**Note:** Make sure to enable GPU: Runtime --> Change runtime type to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I75fyRko1TnJ"
      },
      "source": [
        "## Loading Data\n",
        "The data files used to train the MACE model have to be in `extxyz` format.\n",
        "In this tutorial, we use the 3BPA dataset consisting of 500 configurations sampled a 300K with DFT.\n",
        "The energies are in eV and forces in eV/A."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlpRFJqdn60u",
        "outputId": "5cd80097-56d5-451c-9430-6fab1c944e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'BOTNet-datasets'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 57 (delta 13), reused 37 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (57/57), 28.73 MiB | 17.47 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/davkovacs/BOTNet-datasets.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdZfwzhXzxD8",
        "outputId": "92a3e36a-b3b1-4e7b-9226-5a648d79d2a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iso_atoms.xyz  test_1200K.xyz  test_600K.xyz  train_300K.xyz\n",
            "README.md      test_300K.xyz   test_dih.xyz   train_mixedT.xyz\n"
          ]
        }
      ],
      "source": [
        "!ls BOTNet-datasets/dataset_3BPA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKVohiQJ1P3R"
      },
      "source": [
        "## Training\n",
        "\n",
        "To train a MACE model you can specify the training file with the `--train_file` flag. The validation set can either be specified as a separate file using the `--valid_file` keyword, or it can be specified as a fraction of the training set using the `--valid_fraction` keyword. It is also possible to provide a test set that only gets evaluated at the end of the training using the `--test_file` keyword. If you want to compute the RMSE for different parts of the training set separately, specify the `config_type` keyword in the `info` dict of the configurations.\n",
        "\n",
        "When parsing the data files the energies are read using the keyword `energy` and the forces using the keyword `forces`. To change that, specify the `--energy_key` and `--forces_key`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_NWojkilmE1"
      },
      "source": [
        "For illustration, we create a very small model with 16 invariant messages specified by `hidden_irreps='16x0e'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z10787RE1N8T",
        "outputId": "2720e095-1f81-4ff9-80fe-8dbc7f8c8870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
            "2024-11-12 14:36:07.975 INFO: ===========VERIFYING SETTINGS===========\n",
            "2024-11-12 14:36:07.976 INFO: MACE version: 0.3.7\n",
            "2024-11-12 14:36:07.976 INFO: Using CPU\n",
            "2024-11-12 14:36:08.071 INFO: ===========LOADING INPUT DATA===========\n",
            "2024-11-12 14:36:08.071 INFO: Using heads: ['default']\n",
            "2024-11-12 14:36:08.071 INFO: =============    Processing head default     ===========\n",
            "2024-11-12 14:36:08.534 WARNING: Since ASE version 3.23.0b1, using energy_key 'energy' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'energy' to 'REF_energy'. You need to use --energy_key='REF_energy' to specify the chosen key name.\n",
            "2024-11-12 14:36:08.642 WARNING: Since ASE version 3.23.0b1, using forces_key 'forces' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'forces' to 'REF_forces'. You need to use --forces_key='REF_forces' to specify the chosen key name.\n",
            "2024-11-12 14:36:08.750 INFO: Training set [500 configs, 500 energy, 40500 forces] loaded from 'BOTNet-datasets/dataset_3BPA/train_300K.xyz'\n",
            "2024-11-12 14:36:08.751 INFO: Using random 5% of training set for validation with indices saved in: ./valid_indices_123.txt\n",
            "2024-11-12 14:36:08.751 INFO: Validaton set contains 25 configurations [25 energy, 2025 forces]\n",
            "2024-11-12 14:36:09.637 WARNING: Since ASE version 3.23.0b1, using energy_key 'energy' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'energy' to 'REF_energy'. You need to use --energy_key='REF_energy' to specify the chosen key name.\n",
            "2024-11-12 14:36:09.960 WARNING: Since ASE version 3.23.0b1, using forces_key 'forces' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'forces' to 'REF_forces'. You need to use --forces_key='REF_forces' to specify the chosen key name.\n",
            "2024-11-12 14:36:10.340 INFO: Test set (1669 configs) loaded from 'BOTNet-datasets/dataset_3BPA/test_300K.xyz':\n",
            "2024-11-12 14:36:10.341 INFO: Default_Default: 1669 configs, 1669 energy, 135189 forces\n",
            "2024-11-12 14:36:10.341 INFO: Total number of configurations: train=475, valid=25, tests=[Default_Default: 1669],\n",
            "2024-11-12 14:36:10.345 INFO: Atomic Numbers used: [1, 6, 7, 8]\n",
            "2024-11-12 14:36:10.345 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
            "2024-11-12 14:36:10.345 INFO: Atomic Energies used (z: eV) for head default: {1: -13.663181292231226, 6: -1029.2809654211628, 7: -1484.1187695035828, 8: -2042.0330099956639}\n",
            "2024-11-12 14:36:10.880 INFO: Computing average number of neighbors\n",
            "2024-11-12 14:36:11.044 INFO: Average number of neighbors: 11.992914199829102\n",
            "2024-11-12 14:36:11.044 INFO: During training the following quantities will be reported: energy, forces\n",
            "2024-11-12 14:36:11.044 INFO: ===========MODEL DETAILS===========\n",
            "2024-11-12 14:36:11.161 INFO: Building model\n",
            "2024-11-12 14:36:11.162 INFO: Message passing with 32 channels and max_L=0 (32x0e)\n",
            "2024-11-12 14:36:11.162 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
            "2024-11-12 14:36:11.162 INFO: 8 radial and 5 basis functions\n",
            "2024-11-12 14:36:11.162 INFO: Radial cutoff: 4.0 A (total receptive field for each atom: 8.0 A)\n",
            "2024-11-12 14:36:11.162 INFO: Distance transform for radial basis functions: None\n",
            "2024-11-12 14:36:11.162 INFO: Hidden irreps: 32x0e\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
            "  warnings.warn(\n",
            "2024-11-12 14:36:12.896 INFO: Total number of parameters: 62128\n",
            "2024-11-12 14:36:12.896 INFO: \n",
            "2024-11-12 14:36:12.896 INFO: ===========OPTIMIZER INFORMATION===========\n",
            "2024-11-12 14:36:12.896 INFO: Using ADAM as parameter optimizer\n",
            "2024-11-12 14:36:12.897 INFO: Batch size: 20\n",
            "2024-11-12 14:36:12.897 INFO: Using Exponential Moving Average with decay: 0.99\n",
            "2024-11-12 14:36:12.897 INFO: Number of gradient updates: 2375\n",
            "2024-11-12 14:36:12.897 INFO: Learning rate: 0.01, weight decay: 5e-07\n",
            "2024-11-12 14:36:12.897 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
            "2024-11-12 14:36:12.898 INFO: Stage Two (after 75 epochs) with loss function: WeightedEnergyForcesLoss(energy_weight=1000.000, forces_weight=100.000), with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
            "2024-11-12 14:36:13.022 INFO: Using gradient clipping with tolerance=10.000\n",
            "2024-11-12 14:36:13.022 INFO: \n",
            "2024-11-12 14:36:13.022 INFO: ===========TRAINING===========\n",
            "2024-11-12 14:36:13.023 INFO: Started training, reporting errors on validation set\n",
            "2024-11-12 14:36:13.023 INFO: Loss metrics on validation set\n",
            "2024-11-12 14:36:14.481 INFO: Initial: head: default, loss=11.11455059, RMSE_E_per_atom=   10.55 meV, RMSE_F=  975.98 meV / A\n",
            "2024-11-12 14:36:43.673 INFO: Epoch 0: head: default, loss=3.70784616, RMSE_E_per_atom=  317.56 meV, RMSE_F=  557.62 meV / A\n",
            "2024-11-12 14:37:11.569 INFO: Epoch 1: head: default, loss=2.10038543, RMSE_E_per_atom=  368.76 meV, RMSE_F=  417.03 meV / A\n",
            "2024-11-12 14:37:38.707 INFO: Epoch 2: head: default, loss=1.17339802, RMSE_E_per_atom=  342.98 meV, RMSE_F=  311.33 meV / A\n",
            "2024-11-12 14:38:05.822 INFO: Epoch 3: head: default, loss=0.85164797, RMSE_E_per_atom=  305.73 meV, RMSE_F=  266.16 meV / A\n",
            "2024-11-12 14:38:32.664 INFO: Epoch 4: head: default, loss=0.71036893, RMSE_E_per_atom=  229.32 meV, RMSE_F=  243.67 meV / A\n",
            "2024-11-12 14:38:59.601 INFO: Epoch 5: head: default, loss=0.61717868, RMSE_E_per_atom=  163.47 meV, RMSE_F=  227.32 meV / A\n",
            "2024-11-12 14:39:26.735 INFO: Epoch 6: head: default, loss=0.53051847, RMSE_E_per_atom=  121.90 meV, RMSE_F=  210.80 meV / A\n",
            "2024-11-12 14:39:53.430 INFO: Epoch 7: head: default, loss=0.45799878, RMSE_E_per_atom=   87.84 meV, RMSE_F=  196.05 meV / A\n",
            "2024-11-12 14:40:20.600 INFO: Epoch 8: head: default, loss=0.40751916, RMSE_E_per_atom=   56.90 meV, RMSE_F=  185.04 meV / A\n",
            "2024-11-12 14:40:47.550 INFO: Epoch 9: head: default, loss=0.36931077, RMSE_E_per_atom=   27.21 meV, RMSE_F=  176.35 meV / A\n",
            "2024-11-12 14:41:14.502 INFO: Epoch 10: head: default, loss=0.32962742, RMSE_E_per_atom=    1.93 meV, RMSE_F=  166.76 meV / A\n",
            "2024-11-12 14:41:41.587 INFO: Epoch 11: head: default, loss=0.28755552, RMSE_E_per_atom=   11.92 meV, RMSE_F=  155.86 meV / A\n",
            "2024-11-12 14:42:08.404 INFO: Epoch 12: head: default, loss=0.25214586, RMSE_E_per_atom=   11.82 meV, RMSE_F=  146.03 meV / A\n",
            "2024-11-12 14:42:35.147 INFO: Epoch 13: head: default, loss=0.22509201, RMSE_E_per_atom=    7.80 meV, RMSE_F=  137.83 meV / A\n",
            "2024-11-12 14:43:03.535 INFO: Epoch 14: head: default, loss=0.20240192, RMSE_E_per_atom=    5.28 meV, RMSE_F=  130.57 meV / A\n",
            "2024-11-12 14:43:30.417 INFO: Epoch 15: head: default, loss=0.18268982, RMSE_E_per_atom=    2.84 meV, RMSE_F=  123.83 meV / A\n",
            "2024-11-12 14:43:57.550 INFO: Epoch 16: head: default, loss=0.16290548, RMSE_E_per_atom=    3.62 meV, RMSE_F=  116.67 meV / A\n",
            "2024-11-12 14:44:24.680 INFO: Epoch 17: head: default, loss=0.14455579, RMSE_E_per_atom=    2.02 meV, RMSE_F=  109.67 meV / A\n",
            "2024-11-12 14:44:51.911 INFO: Epoch 18: head: default, loss=0.13126263, RMSE_E_per_atom=    5.20 meV, RMSE_F=  104.23 meV / A\n",
            "2024-11-12 14:45:18.661 INFO: Epoch 19: head: default, loss=0.12163475, RMSE_E_per_atom=    7.75 meV, RMSE_F=  100.20 meV / A\n",
            "2024-11-12 14:45:45.655 INFO: Epoch 20: head: default, loss=0.11371095, RMSE_E_per_atom=    9.77 meV, RMSE_F=   96.78 meV / A\n",
            "2024-11-12 14:46:12.239 INFO: Epoch 21: head: default, loss=0.10667066, RMSE_E_per_atom=    9.59 meV, RMSE_F=   93.58 meV / A\n",
            "2024-11-12 14:46:38.680 INFO: Epoch 22: head: default, loss=0.09996375, RMSE_E_per_atom=    3.82 meV, RMSE_F=   90.43 meV / A\n",
            "2024-11-12 14:47:05.422 INFO: Epoch 23: head: default, loss=0.09387250, RMSE_E_per_atom=    2.93 meV, RMSE_F=   87.53 meV / A\n",
            "2024-11-12 14:47:32.120 INFO: Epoch 24: head: default, loss=0.08858380, RMSE_E_per_atom=    2.87 meV, RMSE_F=   84.92 meV / A\n",
            "2024-11-12 14:47:58.886 INFO: Epoch 25: head: default, loss=0.08374975, RMSE_E_per_atom=    2.79 meV, RMSE_F=   82.54 meV / A\n",
            "2024-11-12 14:48:25.742 INFO: Epoch 26: head: default, loss=0.07954019, RMSE_E_per_atom=    2.36 meV, RMSE_F=   80.38 meV / A\n",
            "2024-11-12 14:48:52.826 INFO: Epoch 27: head: default, loss=0.07589652, RMSE_E_per_atom=    2.48 meV, RMSE_F=   78.48 meV / A\n",
            "2024-11-12 14:49:19.662 INFO: Epoch 28: head: default, loss=0.07263870, RMSE_E_per_atom=    2.43 meV, RMSE_F=   76.70 meV / A\n",
            "2024-11-12 14:49:46.592 INFO: Epoch 29: head: default, loss=0.06958721, RMSE_E_per_atom=    2.80 meV, RMSE_F=   75.03 meV / A\n",
            "2024-11-12 14:50:15.524 INFO: Epoch 30: head: default, loss=0.06699134, RMSE_E_per_atom=    2.46 meV, RMSE_F=   73.61 meV / A\n",
            "2024-11-12 14:50:42.256 INFO: Epoch 31: head: default, loss=0.06452941, RMSE_E_per_atom=    2.05 meV, RMSE_F=   72.21 meV / A\n",
            "2024-11-12 14:51:09.327 INFO: Epoch 32: head: default, loss=0.06212879, RMSE_E_per_atom=    2.40 meV, RMSE_F=   70.84 meV / A\n",
            "2024-11-12 14:51:36.485 INFO: Epoch 33: head: default, loss=0.06008619, RMSE_E_per_atom=    3.60 meV, RMSE_F=   69.62 meV / A\n",
            "2024-11-12 14:52:03.493 INFO: Epoch 34: head: default, loss=0.05814878, RMSE_E_per_atom=    2.82 meV, RMSE_F=   68.49 meV / A\n",
            "2024-11-12 14:52:30.508 INFO: Epoch 35: head: default, loss=0.05643915, RMSE_E_per_atom=    2.38 meV, RMSE_F=   67.44 meV / A\n",
            "2024-11-12 14:52:58.355 INFO: Epoch 36: head: default, loss=0.05478197, RMSE_E_per_atom=    2.22 meV, RMSE_F=   66.43 meV / A\n",
            "2024-11-12 14:53:25.310 INFO: Epoch 37: head: default, loss=0.05324277, RMSE_E_per_atom=    2.14 meV, RMSE_F=   65.48 meV / A\n",
            "2024-11-12 14:53:52.236 INFO: Epoch 38: head: default, loss=0.05177034, RMSE_E_per_atom=    1.98 meV, RMSE_F=   64.55 meV / A\n",
            "2024-11-12 14:54:18.966 INFO: Epoch 39: head: default, loss=0.05042688, RMSE_E_per_atom=    2.03 meV, RMSE_F=   63.70 meV / A\n",
            "2024-11-12 14:54:46.099 INFO: Epoch 40: head: default, loss=0.04913158, RMSE_E_per_atom=    2.21 meV, RMSE_F=   62.86 meV / A\n",
            "2024-11-12 14:55:13.058 INFO: Epoch 41: head: default, loss=0.04784003, RMSE_E_per_atom=    2.46 meV, RMSE_F=   62.05 meV / A\n",
            "2024-11-12 14:55:39.949 INFO: Epoch 42: head: default, loss=0.04670028, RMSE_E_per_atom=    1.84 meV, RMSE_F=   61.31 meV / A\n",
            "2024-11-12 14:56:07.071 INFO: Epoch 43: head: default, loss=0.04566661, RMSE_E_per_atom=    1.64 meV, RMSE_F=   60.64 meV / A\n",
            "2024-11-12 14:56:32.993 INFO: Epoch 44: head: default, loss=0.04452899, RMSE_E_per_atom=    1.75 meV, RMSE_F=   59.87 meV / A\n",
            "2024-11-12 14:57:00.453 INFO: Epoch 45: head: default, loss=0.04353410, RMSE_E_per_atom=    2.06 meV, RMSE_F=   59.19 meV / A\n",
            "2024-11-12 14:57:28.358 INFO: Epoch 46: head: default, loss=0.04267129, RMSE_E_per_atom=    3.19 meV, RMSE_F=   58.61 meV / A\n",
            "2024-11-12 14:57:55.164 INFO: Epoch 47: head: default, loss=0.04175825, RMSE_E_per_atom=    2.03 meV, RMSE_F=   57.99 meV / A\n",
            "2024-11-12 14:58:22.706 INFO: Epoch 48: head: default, loss=0.04083525, RMSE_E_per_atom=    1.59 meV, RMSE_F=   57.38 meV / A\n",
            "2024-11-12 14:58:50.558 INFO: Epoch 49: head: default, loss=0.04005505, RMSE_E_per_atom=    1.17 meV, RMSE_F=   56.82 meV / A\n",
            "2024-11-12 14:59:20.921 INFO: Epoch 50: head: default, loss=0.03928615, RMSE_E_per_atom=    1.08 meV, RMSE_F=   56.31 meV / A\n",
            "2024-11-12 14:59:57.033 INFO: Epoch 51: head: default, loss=0.03852776, RMSE_E_per_atom=    1.03 meV, RMSE_F=   55.76 meV / A\n",
            "2024-11-12 15:00:33.510 INFO: Epoch 52: head: default, loss=0.03776845, RMSE_E_per_atom=    1.35 meV, RMSE_F=   55.23 meV / A\n",
            "2024-11-12 15:01:09.304 INFO: Epoch 53: head: default, loss=0.03713926, RMSE_E_per_atom=    1.03 meV, RMSE_F=   54.78 meV / A\n",
            "2024-11-12 15:01:36.015 INFO: Epoch 54: head: default, loss=0.03644305, RMSE_E_per_atom=    0.98 meV, RMSE_F=   54.29 meV / A\n",
            "2024-11-12 15:02:02.572 INFO: Epoch 55: head: default, loss=0.03586301, RMSE_E_per_atom=    0.94 meV, RMSE_F=   53.86 meV / A\n",
            "2024-11-12 15:02:29.481 INFO: Epoch 56: head: default, loss=0.03529867, RMSE_E_per_atom=    1.14 meV, RMSE_F=   53.45 meV / A\n",
            "2024-11-12 15:02:56.170 INFO: Epoch 57: head: default, loss=0.03469532, RMSE_E_per_atom=    0.95 meV, RMSE_F=   53.01 meV / A\n",
            "2024-11-12 15:03:23.311 INFO: Epoch 58: head: default, loss=0.03414275, RMSE_E_per_atom=    1.17 meV, RMSE_F=   52.61 meV / A\n",
            "2024-11-12 15:03:50.364 INFO: Epoch 59: head: default, loss=0.03363267, RMSE_E_per_atom=    1.55 meV, RMSE_F=   52.25 meV / A\n",
            "2024-11-12 15:04:17.135 INFO: Epoch 60: head: default, loss=0.03313478, RMSE_E_per_atom=    1.71 meV, RMSE_F=   51.89 meV / A\n",
            "2024-11-12 15:04:45.526 INFO: Epoch 61: head: default, loss=0.03269782, RMSE_E_per_atom=    0.84 meV, RMSE_F=   51.56 meV / A\n",
            "2024-11-12 15:05:12.682 INFO: Epoch 62: head: default, loss=0.03223240, RMSE_E_per_atom=    0.86 meV, RMSE_F=   51.22 meV / A\n",
            "2024-11-12 15:05:39.403 INFO: Epoch 63: head: default, loss=0.03184300, RMSE_E_per_atom=    0.80 meV, RMSE_F=   50.90 meV / A\n",
            "2024-11-12 15:06:05.843 INFO: Epoch 64: head: default, loss=0.03150506, RMSE_E_per_atom=    0.82 meV, RMSE_F=   50.64 meV / A\n",
            "2024-11-12 15:06:32.828 INFO: Epoch 65: head: default, loss=0.03116775, RMSE_E_per_atom=    1.14 meV, RMSE_F=   50.39 meV / A\n",
            "2024-11-12 15:07:00.117 INFO: Epoch 66: head: default, loss=0.03078679, RMSE_E_per_atom=    0.87 meV, RMSE_F=   50.08 meV / A\n",
            "2024-11-12 15:07:26.919 INFO: Epoch 67: head: default, loss=0.03033330, RMSE_E_per_atom=    0.77 meV, RMSE_F=   49.73 meV / A\n",
            "2024-11-12 15:07:53.452 INFO: Epoch 68: head: default, loss=0.03003712, RMSE_E_per_atom=    0.74 meV, RMSE_F=   49.50 meV / A\n",
            "2024-11-12 15:08:20.105 INFO: Epoch 69: head: default, loss=0.02965907, RMSE_E_per_atom=    0.80 meV, RMSE_F=   49.21 meV / A\n",
            "2024-11-12 15:08:47.432 INFO: Epoch 70: head: default, loss=0.02921032, RMSE_E_per_atom=    0.70 meV, RMSE_F=   48.87 meV / A\n",
            "2024-11-12 15:09:14.682 INFO: Epoch 71: head: default, loss=0.02881877, RMSE_E_per_atom=    0.80 meV, RMSE_F=   48.54 meV / A\n",
            "2024-11-12 15:09:41.685 INFO: Epoch 72: head: default, loss=0.02846536, RMSE_E_per_atom=    0.77 meV, RMSE_F=   48.26 meV / A\n",
            "2024-11-12 15:10:08.480 INFO: Epoch 73: head: default, loss=0.02816675, RMSE_E_per_atom=    0.76 meV, RMSE_F=   48.02 meV / A\n",
            "2024-11-12 15:10:35.597 INFO: Epoch 74: head: default, loss=0.02781201, RMSE_E_per_atom=    0.72 meV, RMSE_F=   47.73 meV / A\n",
            "2024-11-12 15:10:35.617 INFO: Changing loss based on Stage Two Weights\n",
            "2024-11-12 15:11:02.237 INFO: Epoch 75: head: default, loss=0.02759406, RMSE_E_per_atom=    0.67 meV, RMSE_F=   47.51 meV / A\n",
            "2024-11-12 15:11:28.817 INFO: Epoch 76: head: default, loss=0.02733475, RMSE_E_per_atom=    0.66 meV, RMSE_F=   47.29 meV / A\n",
            "2024-11-12 15:11:57.103 INFO: Epoch 77: head: default, loss=0.02713873, RMSE_E_per_atom=    0.64 meV, RMSE_F=   47.13 meV / A\n",
            "2024-11-12 15:12:23.816 INFO: Epoch 78: head: default, loss=0.02698746, RMSE_E_per_atom=    0.63 meV, RMSE_F=   47.00 meV / A\n",
            "2024-11-12 15:12:50.673 INFO: Epoch 79: head: default, loss=0.02683407, RMSE_E_per_atom=    0.62 meV, RMSE_F=   46.87 meV / A\n",
            "2024-11-12 15:13:17.533 INFO: Epoch 80: head: default, loss=0.02672759, RMSE_E_per_atom=    0.61 meV, RMSE_F=   46.79 meV / A\n",
            "2024-11-12 15:13:44.688 INFO: Epoch 81: head: default, loss=0.02662820, RMSE_E_per_atom=    0.61 meV, RMSE_F=   46.71 meV / A\n",
            "2024-11-12 15:14:11.412 INFO: Epoch 82: head: default, loss=0.02654542, RMSE_E_per_atom=    0.60 meV, RMSE_F=   46.63 meV / A\n",
            "2024-11-12 15:14:37.939 INFO: Epoch 83: head: default, loss=0.02645726, RMSE_E_per_atom=    0.60 meV, RMSE_F=   46.56 meV / A\n",
            "2024-11-12 15:15:05.150 INFO: Epoch 84: head: default, loss=0.02638302, RMSE_E_per_atom=    0.59 meV, RMSE_F=   46.50 meV / A\n",
            "2024-11-12 15:15:31.696 INFO: Epoch 85: head: default, loss=0.02633746, RMSE_E_per_atom=    0.60 meV, RMSE_F=   46.46 meV / A\n",
            "2024-11-12 15:15:58.333 INFO: Epoch 86: head: default, loss=0.02627263, RMSE_E_per_atom=    0.59 meV, RMSE_F=   46.41 meV / A\n",
            "2024-11-12 15:16:25.330 INFO: Epoch 87: head: default, loss=0.02621553, RMSE_E_per_atom=    0.58 meV, RMSE_F=   46.36 meV / A\n",
            "2024-11-12 15:16:51.694 INFO: Epoch 88: head: default, loss=0.02617221, RMSE_E_per_atom=    0.58 meV, RMSE_F=   46.33 meV / A\n",
            "2024-11-12 15:17:18.310 INFO: Epoch 89: head: default, loss=0.02610914, RMSE_E_per_atom=    0.58 meV, RMSE_F=   46.27 meV / A\n",
            "2024-11-12 15:17:45.250 INFO: Epoch 90: head: default, loss=0.02608032, RMSE_E_per_atom=    0.58 meV, RMSE_F=   46.25 meV / A\n",
            "2024-11-12 15:18:12.446 INFO: Epoch 91: head: default, loss=0.02604587, RMSE_E_per_atom=    0.58 meV, RMSE_F=   46.22 meV / A\n",
            "2024-11-12 15:18:39.404 INFO: Epoch 92: head: default, loss=0.02600686, RMSE_E_per_atom=    0.57 meV, RMSE_F=   46.19 meV / A\n",
            "2024-11-12 15:19:07.379 INFO: Epoch 93: head: default, loss=0.02595909, RMSE_E_per_atom=    0.57 meV, RMSE_F=   46.15 meV / A\n",
            "2024-11-12 15:19:34.235 INFO: Epoch 94: head: default, loss=0.02591625, RMSE_E_per_atom=    0.57 meV, RMSE_F=   46.11 meV / A\n",
            "2024-11-12 15:20:01.495 INFO: Epoch 95: head: default, loss=0.02591030, RMSE_E_per_atom=    0.56 meV, RMSE_F=   46.11 meV / A\n",
            "2024-11-12 15:20:28.703 INFO: Epoch 96: head: default, loss=0.02586783, RMSE_E_per_atom=    0.56 meV, RMSE_F=   46.07 meV / A\n",
            "2024-11-12 15:20:55.156 INFO: Epoch 97: head: default, loss=0.02583183, RMSE_E_per_atom=    0.56 meV, RMSE_F=   46.04 meV / A\n",
            "2024-11-12 15:21:22.090 INFO: Epoch 98: head: default, loss=0.02581165, RMSE_E_per_atom=    0.56 meV, RMSE_F=   46.03 meV / A\n",
            "2024-11-12 15:21:49.241 INFO: Epoch 99: head: default, loss=0.02575397, RMSE_E_per_atom=    0.56 meV, RMSE_F=   45.98 meV / A\n",
            "2024-11-12 15:21:49.273 INFO: Training complete\n",
            "2024-11-12 15:21:49.273 INFO: \n",
            "2024-11-12 15:21:49.273 INFO: ===========RESULTS===========\n",
            "2024-11-12 15:21:49.273 INFO: Computing metrics for training, validation, and test sets\n",
            "2024-11-12 15:21:52.588 INFO: Loading checkpoint: ./checkpoints/MACE_model_run-123_epoch-74.pt\n",
            "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(f=checkpoint_info.path, map_location=device),\n",
            "2024-11-12 15:21:52.616 INFO: Loaded Stage one model from epoch 74 for evaluation\n",
            "2024-11-12 15:21:52.617 INFO: Evaluating train_default ...\n",
            "2024-11-12 15:22:00.638 INFO: Evaluating valid_default ...\n",
            "2024-11-12 15:22:01.111 INFO: Error-table on TRAIN and VALID:\n",
            "+---------------+---------------------+------------------+-------------------+\n",
            "|  config_type  | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
            "+---------------+---------------------+------------------+-------------------+\n",
            "| train_default |            0.9      |         46.1     |          4.88     |\n",
            "| valid_default |            0.7      |         47.7     |          4.88     |\n",
            "+---------------+---------------------+------------------+-------------------+\n",
            "2024-11-12 15:22:01.112 INFO: Evaluating Default_Default ...\n",
            "2024-11-12 15:22:35.317 INFO: Error-table on TEST:\n",
            "+-----------------+---------------------+------------------+-------------------+\n",
            "|   config_type   | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
            "+-----------------+---------------------+------------------+-------------------+\n",
            "| Default_Default |            1.0      |         49.5     |          5.16     |\n",
            "+-----------------+---------------------+------------------+-------------------+\n",
            "2024-11-12 15:22:35.318 INFO: Saving model to checkpoints/MACE_model_run-123.model\n",
            "2024-11-12 15:22:35.411 INFO: Compiling model, saving metadata to MACE_model_compiled.model\n",
            "2024-11-12 15:22:36.407 INFO: Loading checkpoint: ./checkpoints/MACE_model_run-123_epoch-99_swa.pt\n",
            "2024-11-12 15:22:36.422 INFO: Loaded Stage two model from epoch 99 for evaluation\n",
            "2024-11-12 15:22:36.423 INFO: Evaluating train_default ...\n",
            "2024-11-12 15:22:44.460 INFO: Evaluating valid_default ...\n",
            "2024-11-12 15:22:45.169 INFO: Error-table on TRAIN and VALID:\n",
            "+---------------+---------------------+------------------+-------------------+\n",
            "|  config_type  | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
            "+---------------+---------------------+------------------+-------------------+\n",
            "| train_default |            0.8      |         44.3     |          4.67     |\n",
            "| valid_default |            0.6      |         46.0     |          4.70     |\n",
            "+---------------+---------------------+------------------+-------------------+\n",
            "2024-11-12 15:22:45.170 INFO: Evaluating Default_Default ...\n",
            "2024-11-12 15:23:18.989 INFO: Error-table on TEST:\n",
            "+-----------------+---------------------+------------------+-------------------+\n",
            "|   config_type   | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
            "+-----------------+---------------------+------------------+-------------------+\n",
            "| Default_Default |            0.8      |         47.8     |          4.98     |\n",
            "+-----------------+---------------------+------------------+-------------------+\n",
            "2024-11-12 15:23:18.989 INFO: Saving model to checkpoints/MACE_model_run-123_stagetwo.model\n",
            "2024-11-12 15:23:19.084 INFO: Compiling model, saving metadata MACE_model_stagetwo_compiled.model\n",
            "2024-11-12 15:23:20.045 INFO: Done\n"
          ]
        }
      ],
      "source": [
        "!python3 ./mace/scripts/run_train.py \\\n",
        "  --name=\"MACE_model\" \\\n",
        "  --train_file=\"BOTNet-datasets/dataset_3BPA/train_300K.xyz\" \\\n",
        "  --valid_fraction=0.05 \\\n",
        "  --forces_key=\"forces\" \\\n",
        "  --energy_key=\"energy\" \\\n",
        "  --test_file=\"BOTNet-datasets/dataset_3BPA/test_300K.xyz\" \\\n",
        "  --E0s='{1:-13.663181292231226, 6:-1029.2809654211628, 7:-1484.1187695035828, 8:-2042.0330099956639}' \\\n",
        "  --model=\"ScaleShiftMACE\" \\\n",
        "  --hidden_irreps='32x0e' \\\n",
        "  --r_max=4.0 \\\n",
        "  --batch_size=20 \\\n",
        "  --max_num_epochs=100 \\\n",
        "  --ema \\\n",
        "  --ema_decay=0.99 \\\n",
        "  --amsgrad \\\n",
        "  --default_dtype=\"float32\" \\\n",
        "  --device=cpu \\\n",
        "  --seed=123 \\\n",
        "  --swa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXeQe8oTPXQ2"
      },
      "source": [
        "It is possible to use `--model=MACE`, in order to have the correct limit for isolated atoms. This is recommanded for task studying bond breaking events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ln2lu5i4aWA"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWKzaxlA2dow"
      },
      "source": [
        "The trained model is realidy usable to run some ASE MD for illustration. The Colab hardware are not very performant so we put a small number of timesteps for illustration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "wfCwdnaWv9rd",
        "outputId": "6747dda2-e2ad-4366-b6fe-3d37ff1ea52d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
            "/usr/local/lib/python3.10/dist-packages/mace/calculators/mace.py:130: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(f=model_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No dtype selected, switching to float32 to match model dtype.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-7dfb3a8dbc2a>:11: DeprecationWarning: Please use atoms.calc = calc\n",
            "  init_conf.set_calculator(calculator)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7dfb3a8dbc2a>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'md_3bpa.xyz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MD finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/md/md.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmaximum\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0msteps\u001b[0m \u001b[0mare\u001b[0m \u001b[0mreached\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \"\"\"\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/optimize/optimize.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \"\"\"\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mconverged\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mirun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconverged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/optimize/optimize.py\u001b[0m in \u001b[0;36mirun\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_converged\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;31m# compute the next step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/md/langevin.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, forces)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# recalc velocities after RATTLE constraints are applied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnd_pos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mforces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_forces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# Update the velocities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/atoms.py\u001b[0m in \u001b[0;36mget_forces\u001b[0;34m(self, apply_constraint, md)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Atoms object has no calculator.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mforces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_forces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapply_constraint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/calculators/abc.py\u001b[0m in \u001b[0;36mget_forces\u001b[0;34m(self, atoms)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_forces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'forces'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matoms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ase/calculators/calculator.py\u001b[0m in \u001b[0;36mget_property\u001b[0;34m(self, name, atoms, allow_calculation)\u001b[0m\n\u001b[1;32m    536\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem_changes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mace/calculators/mace.py\u001b[0m in \u001b[0;36mcalculate\u001b[0;34m(self, atoms, properties, system_changes)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             out = model(\n\u001b[0m\u001b[1;32m    290\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mcompute_stress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_stress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mace/modules/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, training, compute_force, compute_virials, compute_stress, compute_displacement, compute_hessian)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mshifts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shifts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         )\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0medge_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspherical_harmonics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         edge_feats = self.radial_embedding(\n\u001b[1;32m    386\u001b[0m             \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"node_attrs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"edge_index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/e3nn/o3/_spherical_harmonics.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m             ]))\n\u001b[1;32m     95\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'component'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             sh.mul_(torch.cat([\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ls_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/e3nn/o3/_spherical_harmonics.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     94\u001b[0m             ]))\n\u001b[1;32m     95\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'component'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             sh.mul_(torch.cat([\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ls_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from ase import units\n",
        "from ase.md.langevin import Langevin\n",
        "from ase.io import read, write\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from mace.calculators import MACECalculator\n",
        "\n",
        "calculator = MACECalculator(model_paths='/content/checkpoints/MACE_model_run-123.model', device='cpu')\n",
        "init_conf = read('BOTNet-datasets/dataset_3BPA/test_300K.xyz', '0')\n",
        "init_conf.set_calculator(calculator)\n",
        "\n",
        "dyn = Langevin(init_conf, 0.5*units.fs, temperature_K=310, friction=5e-3)\n",
        "def write_frame():\n",
        "        dyn.atoms.write('md_3bpa.xyz', append=True)\n",
        "dyn.attach(write_frame, interval=50)\n",
        "dyn.run(100)\n",
        "print(\"MD finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTsI9MaezNtw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}